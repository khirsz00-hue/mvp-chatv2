/**
 * useVoiceRamble Hook
 * Manages continuous voice recording and live AI parsing for task creation
 * Uses Web Speech API for real-time transcription
 */

import { useState, useRef, useCallback, useMemo, useEffect } from 'react'
import { toast } from 'sonner'
import { debounce } from 'lodash'

interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList
  resultIndex: number
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string
  message: string
}

interface SpeechRecognitionResultList {
  length: number
  item(index: number): SpeechRecognitionResult
  [index: number]: SpeechRecognitionResult
}

interface SpeechRecognitionResult {
  length: number
  item(index: number): SpeechRecognitionAlternative
  [index: number]: SpeechRecognitionAlternative
  isFinal: boolean
}

interface SpeechRecognitionAlternative {
  transcript: string
  confidence: number
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean
  interimResults: boolean
  lang: string
  onresult: ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any) | null
  onerror: ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any) | null
  onend: ((this: SpeechRecognition, ev: Event) => any) | null
  start(): void
  stop(): void
  abort(): void
}

declare global {
  interface Window {
    SpeechRecognition: new () => SpeechRecognition
    webkitSpeechRecognition: new () => SpeechRecognition
  }
}

export interface ParsedTask {
  title: string
  due_date: string | null
  estimate_min: number
  context_type: string
}

interface ParseResponse {
  action: 'ADD_TASKS' | 'UNDO' | 'CANCEL_ALL'
  tasks: ParsedTask[]
  message?: string
}

export function useVoiceRamble() {
  const [isRecording, setIsRecording] = useState(false)
  const [liveTranscription, setLiveTranscription] = useState('')
  const [parsedTasks, setParsedTasks] = useState<ParsedTask[]>([])
  const [lastAction, setLastAction] = useState<string | null>(null)
  const [isProcessing, setIsProcessing] = useState(false)
  const [retryCount, setRetryCount] = useState(0)

  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const fullTranscriptRef = useRef('')
  const isRecordingRef = useRef(false) // Use ref to avoid stale closure in onend
  const retryCountRef = useRef(0) // Track retry count in ref for onend callback

  // Check browser compatibility
  const isSpeechRecognitionSupported = useMemo(() => {
    return (
      typeof window !== 'undefined' &&
      ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)
    )
  }, [])

  // Parse transcript with AI (debounced)
  const parseTranscript = useCallback(async (transcript: string) => {
    if (!transcript.trim()) return

    setIsProcessing(true)
    setLastAction(null)

    try {
      const response = await fetch('/api/voice/parse-ramble', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          transcript,
          existingTasks: parsedTasks
        })
      })

      if (!response.ok) {
        throw new Error('Failed to parse transcript')
      }

      const data: ParseResponse = await response.json()

      if (data.action === 'UNDO') {
        setParsedTasks(prev => {
          if (prev.length === 0) return prev
          const removed = prev[prev.length - 1]
          setLastAction(`‚ö†Ô∏è Cofniƒôto: "${removed.title}"`)
          return prev.slice(0, -1)
        })
      } else if (data.action === 'CANCEL_ALL') {
        handleCancelAll()
      } else if (data.action === 'ADD_TASKS') {
        setParsedTasks(data.tasks)
        if (data.message) {
          setLastAction(data.message)
        }
      }
    } catch (error) {
      console.error('‚ùå [Voice Ramble] Parse error:', error)
      toast.error('Nie uda≈Ço siƒô przetworzyƒá dyktowania')
    } finally {
      setIsProcessing(false)
    }
  }, [parsedTasks])

  // Debounced parse - triggers 1.5s after user stops talking
  const debouncedParse = useMemo(
    () => debounce(parseTranscript, 1500),
    [parseTranscript]
  )

  // Start recording
  const startRecording = useCallback(() => {
    if (!isSpeechRecognitionSupported) {
      toast.error(
        'Twoja przeglƒÖdarka nie wspiera dyktowania. U≈ºyj Chrome lub Edge.',
        { duration: 5000 }
      )
      return
    }

    try {
      const SpeechRecognitionAPI =
        window.SpeechRecognition || window.webkitSpeechRecognition

      const recognition = new SpeechRecognitionAPI()
      recognition.lang = 'pl-PL'
      recognition.continuous = true
      recognition.interimResults = true

      recognition.onresult = (event: SpeechRecognitionEvent) => {
        let interimTranscript = ''
        let finalTranscript = ''

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript + ' '
          } else {
            interimTranscript += transcript
          }
        }

        if (finalTranscript) {
          fullTranscriptRef.current += finalTranscript
        }

        const displayTranscript = fullTranscriptRef.current + interimTranscript
        setLiveTranscription(displayTranscript)

        // Trigger parsing when user stops talking (debounced)
        if (finalTranscript) {
          debouncedParse(fullTranscriptRef.current)
        }
      }

      recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
        console.error('‚ùå [Voice Ramble] Speech recognition error:', event.error)
        
        // Map errors to user-friendly messages
        const errorMessages: Record<string, string> = {
          'network': 'B≈ÇƒÖd po≈ÇƒÖczenia. Sprawd≈∫ internet i spr√≥buj ponownie.',
          'not-allowed': 'Brak dostƒôpu do mikrofonu. Zezw√≥l w ustawieniach przeglƒÖdarki.',
          'no-speech': 'Nie wykryto mowy. Spr√≥buj m√≥wiƒá g≈Ço≈õniej.',
          'aborted': 'Nagrywanie przerwane.',
          'audio-capture': 'Nie wykryto mikrofonu.',
          'service-not-allowed': 'Us≈Çuga rozpoznawania mowy niedostƒôpna.'
        }
        
        // Only show error toast for critical errors or repeated network errors
        if (event.error === 'no-speech') {
          // Don't show error for no-speech, it's normal during pauses
          return
        }
        
        if (event.error === 'network') {
          // Retry on network errors (max 3 attempts)
          if (retryCountRef.current < 3) {
            console.log(`üîÑ [Voice Ramble] Retrying... (${retryCountRef.current + 1}/3)`)
            retryCountRef.current += 1
            setRetryCount(retryCountRef.current)
            
            // Wait 1 second before retry
            setTimeout(() => {
              if (isRecordingRef.current && recognitionRef.current) {
                try {
                  recognitionRef.current.start()
                } catch (error) {
                  console.error('‚ùå [Voice Ramble] Failed to restart after network error:', error)
                }
              }
            }, 1000)
            return
          } else {
            // Max retries exceeded - show error
            toast.error(errorMessages[event.error])
            setIsRecording(false)
            isRecordingRef.current = false
          }
        } else if (['not-allowed', 'service-not-allowed', 'audio-capture'].includes(event.error)) {
          // Critical errors - stop recording and show message
          toast.error(errorMessages[event.error], { duration: 5000 })
          setIsRecording(false)
          isRecordingRef.current = false
        } else {
          // Other errors - show generic message
          const message = errorMessages[event.error] || 'WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd'
          toast.error(message)
        }
      }

      recognition.onend = () => {
        console.log('üîç [Voice Ramble] Recognition ended')
        // Auto-restart if still recording (check ref to avoid stale closure)
        if (isRecordingRef.current && recognitionRef.current) {
          try {
            // Reset retry count on successful restart
            retryCountRef.current = 0
            setRetryCount(0)
            recognition.start()
          } catch (error) {
            console.error('‚ùå [Voice Ramble] Failed to restart:', error)
          }
        }
      }

      recognition.start()
      recognitionRef.current = recognition
      setIsRecording(true)
      isRecordingRef.current = true
      retryCountRef.current = 0 // Reset retry count on new recording
      setRetryCount(0)
      fullTranscriptRef.current = ''
      setLiveTranscription('')
      setParsedTasks([])
      setLastAction(null)

      console.log('‚úÖ [Voice Ramble] Recording started')
    } catch (error) {
      console.error('‚ùå [Voice Ramble] Failed to start recording:', error)
      toast.error('Nie uda≈Ço siƒô uruchomiƒá nagrywania')
    }
  }, [isSpeechRecognitionSupported, debouncedParse])

  // Stop recording
  const stopRecording = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop()
      recognitionRef.current = null
    }
    setIsRecording(false)
    isRecordingRef.current = false
    debouncedParse.cancel()
    console.log('‚èπÔ∏è [Voice Ramble] Recording stopped')
  }, [debouncedParse])

  // Cancel all tasks
  const handleCancelAll = useCallback(() => {
    stopRecording()
    setParsedTasks([])
    setLiveTranscription('')
    fullTranscriptRef.current = ''
    setLastAction(null)
    toast.info('Anulowano wszystkie zadania')
  }, [stopRecording])

  // Save all tasks
  const handleSaveAll = useCallback(async () => {
    if (parsedTasks.length === 0) {
      toast.error('Brak zada≈Ñ do zapisania')
      return
    }

    stopRecording()

    try {
      const response = await fetch('/api/voice/save-tasks', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ tasks: parsedTasks })
      })

      if (!response.ok) {
        throw new Error('Failed to save tasks')
      }

      const { saved } = await response.json()

      toast.success(`‚úÖ Dodano ${saved} ${saved === 1 ? 'zadanie' : 'zada≈Ñ'} g≈Çosem`)

      // Dispatch event for queue refresh
      window.dispatchEvent(new CustomEvent('voice-tasks-saved'))

      // Reset state
      setParsedTasks([])
      setLiveTranscription('')
      fullTranscriptRef.current = ''
      setLastAction(null)

      return true
    } catch (error) {
      console.error('‚ùå [Voice Ramble] Save error:', error)
      toast.error('Nie uda≈Ço siƒô zapisaƒá zada≈Ñ')
      return false
    }
  }, [parsedTasks, stopRecording])

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      debouncedParse.cancel()
    }
  }, [debouncedParse])

  return {
    isRecording,
    liveTranscription,
    parsedTasks,
    lastAction,
    isProcessing,
    isSpeechRecognitionSupported,
    startRecording,
    stopRecording,
    handleCancelAll,
    handleSaveAll
  }
}
