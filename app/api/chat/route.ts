import { NextResponse } from 'next/server'

// âœ… Uruchamiamy tylko po stronie serwera
export const runtime = 'nodejs'

export async function POST(req: Request) {
  try {
    const { message, context } = await req.json()

    // ğŸ§© Walidacja wejÅ›cia
    if (typeof message !== 'string' || message.trim().length === 0) {
      return NextResponse.json(
        { error: 'NieprawidÅ‚owa wiadomoÅ›Ä‡ â€” oczekiwano tekstu.' },
        { status: 400 }
      )
    }

    if (!process.env.OPENAI_API_KEY) {
      console.error('âŒ Brak OPENAI_API_KEY w Å›rodowisku!')
      return NextResponse.json(
        { error: 'Brak konfiguracji OpenAI API Key.' },
        { status: 500 }
      )
    }

    // âœ… Dynamiczny import klienta OpenAI
    const OpenAI = (await import('openai')).default
    const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

    // ğŸ’¬ Budujemy kontekstowy prompt
    const systemPrompt = [
      'JesteÅ› asystentem produktywnoÅ›ci, ktÃ³ry pomaga uÅ¼ytkownikowi wykonaÄ‡ zadanie.',
      'Zawsze pytaj o szczegÃ³Å‚y, jeÅ›li coÅ› nie jest jasne, i odpowiadaj praktycznie, zwiÄ™Åºle i po polsku.',
      context ? `Kontekst zadania: ${context}` : '',
    ]
      .filter(Boolean)
      .join('\n')

    // ğŸ§  WywoÅ‚anie modelu OpenAI
    const completion = await client.chat.completions.create({
      model: 'gpt-4o-mini',
      temperature: 0.7,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: message.trim() },
      ],
    })

    const reply =
      completion.choices?.[0]?.message?.content?.trim() ||
      'Brak odpowiedzi od modelu.'

    // âœ… Zwracamy poprawnÄ… odpowiedÅº
    return NextResponse.json({ reply })
  } catch (error: any) {
    console.error('âŒ BÅ‚Ä…d w /api/chat:', error)

    return NextResponse.json(
      {
        error:
          error?.response?.data?.error?.message ||
          error.message ||
          'WewnÄ™trzny bÅ‚Ä…d serwera przy komunikacji z OpenAI.',
      },
      { status: 500 }
    )
  }
}
